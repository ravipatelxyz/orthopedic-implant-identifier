{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T10:29:41.486071Z",
     "start_time": "2020-03-19T10:29:39.504837Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify paramaters and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of images you're want to use train - i.e. UNIL (unilateral squared radiographs) or CROP (segmented images)\n",
    "IMAGE_TYPE = \"UNIL\"\n",
    "\n",
    "# Size of balanced test and validation datasets\n",
    "TEST_IMAGES_PER_CLASS = 15\n",
    "VALID_IMAGES_PER_CLASS = 10\n",
    "\n",
    "# Directories\n",
    "INPUT_DIR = \"\"\n",
    "OUTPUT_DIR = \"\"\n",
    "\n",
    "# Path and name to excel file containing segmentation training and test datasets (\"Unet_train_test.xlsx\")...\n",
    "# ...so that images used to train unet won't be included in classification training and testing sets...\n",
    "# ...otherwise might favourably bias results\n",
    "SEGMENTATION_DATASET_FILE_PATH = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a dataframe containing all images with model labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T14:58:28.143294Z",
     "start_time": "2020-03-18T14:58:27.939352Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a list of all filenames\n",
    "\n",
    "os.chdir(INPUT_DIR)\n",
    "filenames = os.listdir()\n",
    "for file in filenames:\n",
    "    if file[-8:] != f\"{IMAGE_TYPE}.png\":\n",
    "        filenames.remove(file)\n",
    "\n",
    "# make a list of all labels (integer based labels)\n",
    "\n",
    "int_labels_all = []\n",
    "for filename in filenames:\n",
    "    label = filename[0:2]\n",
    "    int_labels_all.append(label)\n",
    "\n",
    "# make a list of all labels (string versions)\n",
    "    \n",
    "labels_all = []\n",
    "if label == \"01\":\n",
    "        labels_all.append(\"Hip_DepuySynthes_Corail_Collar\")\n",
    "    elif label == \"02\":\n",
    "        labels_all.append(\"Hip_DepuySynthes_Corail_NilCol\")\n",
    "    elif label == \"03\":\n",
    "        labels_all.append(\"Hip_JRIOrtho_FurlongEvolution_Collar\")\n",
    "    elif label == \"04\":\n",
    "        labels_all.append(\"Hip_JRIOrtho_FurlongEvolution_Collar\")\n",
    "    elif label == \"05\":\n",
    "        labels_all.append(\"Hip_SmithAndNephew_Anthology\")\n",
    "    elif label == \"06\":\n",
    "        labels_all.append(\"Hip_SmithAndNephew_Polarstem_NilCol\")\n",
    "    elif label == \"07\":\n",
    "        labels_all.append(\"Hip_Stryker_AccoladeII\")\n",
    "    elif label == \"08\":\n",
    "        labels_all.append(\"Hip_Stryker_Exeter\")\n",
    "    elif label == \"21\":\n",
    "        labels_all.append(\"Knee_Depuy_Synthes_Sigma\")\n",
    "    elif label == \"22\":\n",
    "        labels_all.append(\"Knee_SmithAndNephew_GenesisII\")\n",
    "    elif label == \"23\":\n",
    "        labels_all.append(\"Knee_SmithAndNephew_Legion\")\n",
    "    elif label == \"24\":\n",
    "        labels_all.append(\"Knee_ZimmerBiomet_Oxford\")\n",
    "\n",
    "# Make a dataframe of filenames and corresponding labels\n",
    "\n",
    "data_tuples = list(zip(filenames, labels_all))\n",
    "full_dataset = pd.DataFrame(data_tuples, columns = ['filenames','labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T14:58:28.295760Z",
     "start_time": "2020-03-18T14:58:28.151268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print a count of images per implant model class\n",
    "    \n",
    "count_of_labels = full_dataset.groupby('labels').count()\n",
    "print(\"Count of all labels\")\n",
    "print(count_of_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating test dataset\n",
    "##### This dataset will be reserved for final classification model testing only, to avoid information leak from hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T14:58:56.603712Z",
     "start_time": "2020-03-18T14:58:28.299765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create list of images used to train unet, so that can ensure these are not included in test dataset\n",
    "unet_images = pd.read_excel(SEGMENTATION_DATASET_FILE_PATH, \n",
    "                            sheet_name=\"training\", \n",
    "                            usecols=[\"filenames\"], \n",
    "                            dtype=str).to_numpy()\n",
    "unet_image = unet_images[:,0]\n",
    "\n",
    "# List of all model classes\n",
    "classes = list(set(full_dataset['labels']))\n",
    "\n",
    "# Function to pull a random set of entries from a list\n",
    "def list_random_numbers_inrange(rand_range, list_length, seed=5):\n",
    "    random.seed(seed)\n",
    "    random_list = random.sample(range(1,rand_range-1),list_length)\n",
    "    return random_list                           \n",
    "\n",
    "# Create test dataframe\n",
    "test_df = pd.DataFrame()\n",
    "training_df = full_dataset\n",
    "\n",
    "for c in classes:\n",
    "    # Generate list of row indices for all images in class used to train unet\n",
    "    unet_image_indices = [index for index, row in full_dataset.iterrows() if row[1] in unet_images]\n",
    "    # Generate a list of row indices for all of a particular class\n",
    "    class_indices = [index for index, row in full_dataset.iterrows() if row[1] == c]\n",
    "    class_indices_excluding_unet = [index for index in class_indices if not index in unet_image_indices]\n",
    "    # Pull 15 random entries from the list (i.e. corresponding to 15 images from that class)\n",
    "    class_indices_selection = list_random_numbers_inrange(list_length=TEST_IMAGES_PER_CLASS, \n",
    "                                                          rand_range = len(class_indices_excluding_unet), \n",
    "                                                          seed=6)\n",
    "    class_indices_test = [class_indices_excluding_unet[i] for i in class_indices_selection]\n",
    "    # Pull and append rows with those indices from full_dataset to a new dataframe of test examples\n",
    "    test_df = test_df.append(full_dataset.iloc[class_indices_test])\n",
    "    # Remove those indices to leave behind a dataframe of training examples\n",
    "    training_df = training_df.drop(index = class_indices_test)\n",
    "\n",
    "print(\"Dataframe containing separated balanced test dataset has been generated: test_df\")\n",
    "print(\"The number of images in the test dataset is:\")\n",
    "print(f\"{len(test_df)} images in total\")\n",
    "print(\"or\")\n",
    "print(f\"{len(test_df)/len(classes)} images per class\")\n",
    "print(f\"training_df now has: {len(training_df)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T14:58:56.641922Z",
     "start_time": "2020-03-18T14:58:56.611713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-index train_df so that there are no skipped indices (otherwise results in empty rows in future dataframes)\n",
    "training_df.reset_index(inplace=True)\n",
    "training_df.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "files_train = training_df['filenames']\n",
    "labels_train = training_df['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T14:59:21.698533Z",
     "start_time": "2020-03-18T14:58:56.649951Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of all included classes\n",
    "classes = list(set(training_df['labels']))\n",
    "\n",
    "# Create validation dataframe\n",
    "valid_df = pd.DataFrame()\n",
    "train_df = training_df\n",
    "\n",
    "for c in classes:\n",
    "    # Generate list of row indices for all images in class used to train unet\n",
    "    unet_image_indices = [index for index, row in EXAMPLES_TO_INCLUDE_DF.iterrows() if row[1] in unet_images]\n",
    "    # Generate a list of row indices for all of a particular class\n",
    "    class_indices = [index for index, row in training_df.iterrows() if row[1] == c]\n",
    "    class_indices_excluding_unet = [index for index in class_indices if not index in unet_image_indices]\n",
    "    # Pull 10 random entries from the list (i.e. corresponding to 10 images from that class)\n",
    "    class_indices_selection = list_random_numbers_inrange(list_length=VALID_IMAGES_PER_CLASS, \n",
    "                                                          rand_range = len(class_indices_excluding_unet), \n",
    "                                                          seed=7)\n",
    "    class_indices_valid = [class_indices[i] for i in class_indices_selection]\n",
    "    # Pull and append rows with those indices from train_df to a new dataframe of validation examples\n",
    "    valid_df = valid_df.append(training_df.iloc[class_indices_valid])\n",
    "    # Remove those indices to leave behind a dataframe of training examples\n",
    "    train_df = train_df.drop(index = class_indices_valid)\n",
    "\n",
    "print(\"Dataframe containing separated validation dataset has been generated: valid_df\")\n",
    "print(\"The number of images in the validation dataset is:\")\n",
    "print(f\"{len(valid_df)} images in total\")\n",
    "print(\"or\")\n",
    "print(f\"{len(valid_df)/len(classes)} images per class\")\n",
    "print(f\"train_df now has: {len(train_df)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting training, validation, and test dataframes into an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T14:59:23.514097Z",
     "start_time": "2020-03-18T14:59:22.178407Z"
    }
   },
   "outputs": [],
   "source": [
    "# save training and validation dataframes to an excel sheet\n",
    "if not os.path.exists(f'classification_training_validation_test_{IMAGE_TYPE}.xlsx'):\n",
    "    with pd.ExcelWriter(f'classification_training_validation_test_{IMAGE_TYPE}.xlsx') as writer:\n",
    "        # training dataset during hyperparameter optimisation\n",
    "        train_df.to_excel(writer, sheet_name='TRAIN_DF', index=False) \n",
    "        # validation dataset for hyperaparameter optimisation\n",
    "        valid_df.to_excel(writer, sheet_name='VALID_DF', index=False)\n",
    "        # combined train and val sets\n",
    "        training_df.to_excel(writer, sheet_name=\"FINAL_TRAIN_DF\", index=False)\n",
    "        # reserved dataset for final testing\n",
    "        test_df.to_excel(writer, sheet_name=\"TEST_DF\", index=False) \n",
    "else:\n",
    "     print(f\"IMPORTANT: classification_training_validation_test_{IMAGE_TYPE}.xlsx already exists in the directory, if you want to overwrite it you need to manually delete the current version first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "354.2px",
    "left": "1143.8px",
    "right": "20px",
    "top": "120px",
    "width": "372.2px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
