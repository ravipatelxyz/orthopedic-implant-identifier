{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9a38271e-8966-4c15-8364-f3f6e51c2d90",
    "_uuid": "dddbc24c-c5a7-4510-8bdd-552272b064a9"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "20e00d93-fb88-417c-83a7-677591bd4af4",
    "_uuid": "4a847884-6f13-4fe8-bfb3-08565098a0a9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import skimage as sk\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import History, EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify parameters and directories\n",
    "Decide on the following experimental variables before running the rest of this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment details\n",
    "EXPERIMENT_ID = \"0001\" #this will autofill\n",
    "IMAGE_TYPE = \"CROP\" #select from UNIL or CROP\n",
    "\n",
    "# Neural net details\n",
    "ARCHIT = 'Xception'\n",
    "PRETRAINED_WEIGHTS = 'imagenet'\n",
    "MONITORED_METRIC = 'loss'\n",
    "## Set as TRAIN_DF during hyperparameter optimisation, and FINAL_TRAIN_DF in final testing:\n",
    "TRAINING_DATASET = \"FINAL_TRAIN_DF\"\n",
    "## Set as VALID_DF during hyperparameter optimisation, and TEST_DF in final testing:\n",
    "EVALUATION_DATASET = \"TEST_DF\" \n",
    "\n",
    "# Training and validation details\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 1 #keep this to 1 if possible, otherwise not all images in validation dataset will be used (unless number of val images is multiple of batch size)\n",
    "NUMBER_EPOCHS = 40 #specify maximum number of epochs to train for (unless early stopping)\n",
    "\n",
    "# Set directories\n",
    "INPUT_DIR = \"\" # Path for folder containing input images (either UNIL or CROP images accordingly)\n",
    "OUTPUT_DIR = \"\" # Path for folder in which you would like to save output files\n",
    "os.chdir(INPUT_DIR)\n",
    "\n",
    "# Update this filepath to the Excel file containing classification train/val/test splitting\n",
    "TRAIN_VALID_TEST_EXCEL_FILEPATH = f\"............/classification_training_validation_test_{IMAGE_TYPE}.xlsx\"\n",
    "TRAIN_DF = pd.read_excel(TRAIN_VALID_TEST_EXCEL_FILEPATH, sheet_name=TRAINING_DATASET, dtype=str)\n",
    "EVAL_DF = pd.read_excel(TRAIN_VALID_TEST_EXCEL_FILEPATH, sheet_name=EVALUATION_DATASET, dtype=str)\n",
    "print(TRAIN_DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bbe9b1ca-f895-48ee-9beb-d5ea78011597",
    "_uuid": "1907b0de-48ff-495c-8e42-4790e4710119"
   },
   "source": [
    "## Set up the tools to load the data\n",
    "For the training data we will use augmentation, but not for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5f4cd201-8646-4618-90ef-3263bed67564",
    "_uuid": "29f1b0a4-28f5-4d55-a617-51e544c6de5a"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=180,\n",
    "                             width_shift_range=0.15,\n",
    "                             height_shift_range=0.15,\n",
    "                             rescale=1. / 255,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.15,\n",
    "                             horizontal_flip=True,\n",
    "                             brightness_range=(0.2, 2.0),\n",
    "                             fill_mode='nearest')\n",
    "eval_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(TRAIN_DF, directory=None, \n",
    "                                                    class_mode='categorical', \n",
    "                                                    x_col='filenames', \n",
    "                                                    y_col='labels',\n",
    "                                                    target_size=(299, 299), \n",
    "                                                    batch_size=TRAIN_BATCH_SIZE, \n",
    "                                                    shuffle=True)\n",
    "eval_generator = eval_datagen.flow_from_dataframe(EVAL_DF, \n",
    "                                                    directory=None, \n",
    "                                                    class_mode='categorical', \n",
    "                                                    x_col='filenames', \n",
    "                                                    y_col='labels',\n",
    "                                                    target_size=(299, 299), \n",
    "                                                    batch_size=EVAL_BATCH_SIZE, \n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c5cb3d6c-aefd-4655-b1ef-a3686e666428",
    "_uuid": "bfec04ce-d724-48b7-b776-bc4f70592d66"
   },
   "source": [
    "## Calculate steps per epoch\n",
    "\n",
    "Because we're 'streaming' files from the disk rather than loading them all into memory at once, we need tell keras how many times it will need to pull data to get through the entire dataset once (how many steps per epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ab0de8f2-39ef-45a1-b402-fe3ff2544cfc",
    "_uuid": "f64964ad-019d-4704-9c7f-f342e564cb3f"
   },
   "outputs": [],
   "source": [
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "step_size_eval=eval_generator.n//eval_generator.batch_size\n",
    "\n",
    "num_classes = TRAIN_DF['labels'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "33dc0830-a767-437d-98a5-5ad941dd89fc",
    "_uuid": "eab37449-ffb4-4108-8d3c-2da12618003e"
   },
   "source": [
    "## Create the model\n",
    " Load the network and create a new 'output' layer with the correct number of neurons (1 per model of implant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "725c93e0-d6d1-4b63-baf2-da17e6c64e05",
    "_uuid": "a35d19d9-2f29-4e9f-bc42-88b505ae0ed4"
   },
   "outputs": [],
   "source": [
    "architecture = Xception(include_top=False, \n",
    "                        weights=PRETRAINED_WEIGHTS, \n",
    "                        pooling='avg', \n",
    "                        input_shape=(299, 299, 3), \n",
    "                        classes=num_classes)\n",
    "predictions = Dense(num_classes, activation='softmax')(architecture.output)\n",
    "\n",
    "model = Model(inputs=architecture.input, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model checkpoints\n",
    "- Save the model at the final training epoch (mc2) and at the epoch with the best value for our monitored metric (mc1).\n",
    "- In our paper, we used only the mc2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpoints to save model at intervals\n",
    "mc1 = ModelCheckpoint(f\"OUTPUT_DIR/{EXPERIMENT_ID}_{ARCHIT}_{PRETRAINED_WEIGHTS}_{IMAGE_TYPE}_BEST{MONITORED_METRIC}.h5\", \n",
    "                     monitor=MONITORED_METRIC, mode='auto', verbose=1, save_best_only=True)   # to save best metric\n",
    "mc2 = ModelCheckpoint(f\"OUTPUT_DIR/{EXPERIMENT_ID}_{ARCHIT}_{PRETRAINED_WEIGHTS}_{IMAGE_TYPE}_FINALEPOCH.h5\", \n",
    "                     monitor=MONITORED_METRIC, mode='auto', verbose=1, save_best_only=False, period=5)   # to save last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4dbdd38b-8d76-42a1-baeb-7cc5c980f38e",
    "_uuid": "c52fda13-2e04-421f-a269-071b9b5fd678"
   },
   "source": [
    "## Training\n",
    "Train for specified number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "45bd8ae8-008f-49c3-be96-f698ac9e06c4",
    "_uuid": "7196357d-f5f3-430a-8312-ac0c71e2e414"
   },
   "outputs": [],
   "source": [
    "# weight balancing to compensate for imbalanced model class sizes\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "           'balanced',\n",
    "            np.unique(train_generator.classes), \n",
    "            train_generator.classes)\n",
    "# fit model\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                steps_per_epoch=step_size_train,\n",
    "                validation_data=eval_generator,\n",
    "                validation_steps=step_size_eval,\n",
    "                epochs=NUMBER_EPOCHS,\n",
    "                class_weight=class_weights,\n",
    "                callbacks=[mc1, mc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training accuracy values\n",
    "plt.plot(history.history['accuracy'], color='k', linestyle='-')\n",
    "plt.plot(history.history['val_accuracy'], color='r', linestyle='-')\n",
    "plt.title(f'{ARCHIT} model accuracy',  color='k')\n",
    "plt.ylabel('Accuracy',  color='k')\n",
    "plt.xlabel('Epoch',  color='k')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.tick_params(colors='k')\n",
    "plt.xlim(0, NUMBER_EPOCHS)\n",
    "plt.ylim(top=1)\n",
    "plt.savefig(f\"OUTPUT_DIR/{ARCHIT}_{IMAGE_TYPE}_accuracy_curve_{EXPERIMENT_ID}.png\", \n",
    "            dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training loss values\n",
    "plt.plot(history.history['loss'], color='k', linestyle='-')\n",
    "plt.plot(history.history['val_loss'], color='r', linestyle='-')\n",
    "plt.title(f'{ARCHIT} model loss', color='k')\n",
    "plt.ylabel('Loss', color='k')\n",
    "plt.xlabel('Epoch', color='k')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.tick_params(colors='k')\n",
    "plt.xlim(0, NUMBER_EPOCHS)\n",
    "plt.ylim(bottom=0)\n",
    "plt.savefig(f\"OUTPUT_DIR/{ARCHIT}_{IMAGE_TYPE}_loss_curve_{EXPERIMENT_ID}.png\", \n",
    "            dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ff9e8e1-f1b3-45d4-822e-f311825b70fc",
    "_uuid": "cb1978a4-56fb-4374-8ace-d3ba6afd78cd"
   },
   "source": [
    "## Evaluation\n",
    "Numerical evaluation: conusion matrix, accuracy, F1 score, top 3 accuracy, trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image generator for validation images to feed into further evaluation of network performance\n",
    "eval_generator = eval_datagen.flow_from_dataframe(EVAL_DF,\n",
    "                                                   directory=None,\n",
    "                                                   class_mode='categorical',\n",
    "                                                   x_col='filenames',\n",
    "                                                   y_col='labels',\n",
    "                                                   target_size=(299, 299),\n",
    "                                                   batch_size=EVAL_BATCH_SIZE,\n",
    "                                                   shuffle=False)\n",
    "\n",
    "# load best model\n",
    "\n",
    "saved_model = load_model(f\"OUTPUT_DIR/{EXPERIMENT_ID}_{ARCHIT}_{PRETRAINED_WEIGHTS}_{IMAGE_TYPE}_BEST{MONITORED_METRIC}.h5\")\n",
    "\n",
    "# Generate dataframe of predicted labels and true labels\n",
    "\n",
    "y_pred_max_list = []\n",
    "y_true_max_list = []\n",
    "\n",
    "batches = 0\n",
    "for x_batch, y_batch in eval_generator:\n",
    "    y_pred = saved_model.predict(x_batch)\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "    y_pred_max_list.extend(y_pred_max)\n",
    "    y_true_max = np.argmax(y_batch, axis=1)\n",
    "    y_true_max_list.extend(y_true_max)\n",
    "    batches += 1\n",
    "    if batches >= eval_generator.n/eval_generator.batch_size: #note calculate number of batches by dividing the number of images in validation set by batch_size of eval_generator\n",
    "        break\n",
    "        \n",
    "prediction_df = pd.DataFrame({'predictions': y_pred_max_list,'labels': y_true_max_list})\n",
    "  \n",
    "# Generate confusion matrix numpy array\n",
    "confusion_mx = confusion_matrix(y_true_max_list,y_pred_max_list)\n",
    "\n",
    "# Then convert confusion matrix into pd dataframe with labels along column and along top\n",
    "class_indices = eval_generator.class_indices\n",
    "confusion_matrix_df = pd.DataFrame(confusion_mx,\n",
    "                                   index = [f\"{i}_{c}_true\" for i,c in enumerate(list(class_indices.keys()))],\n",
    "                                   columns = [f\"{i}\" for i,c in enumerate(list(class_indices.keys()))])\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix_df)\n",
    "\n",
    "# Export confusion matrix to CSV file\n",
    "confusion_matrix_df.to_csv(f\"OUTPUT_DIR/{EXPERIMENT_ID}_{ARCHIT}_{PRETRAINED_WEIGHTS}_{IMAGE_TYPE}_confusion_matrix_BEST{MONITORED_METRIC}.csv\")\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = accuracy_score(y_true_max_list,y_pred_max_list)\n",
    "print(f\"\\nAccuracy: {overall_accuracy}\")\n",
    "\n",
    "# F1 score\n",
    "f1score = f1_score(y_true_max_list,y_pred_max_list, average='weighted')\n",
    "print(f\"\\nF1 score: {f1score}\")\n",
    "\n",
    "# Top 3 categorical accuracy\n",
    "\n",
    "top_3_accuracy_list = []\n",
    "\n",
    "batches = 0\n",
    "for x_batch, y_batch in eval_generator:\n",
    "    y_pred = saved_model.predict(x_batch)[0]\n",
    "    top_3_pred = np.argpartition(y_pred, -3)[-3:]\n",
    "    if top_3_pred[0] == np.argmax(y_batch) or top_3_pred[1] == np.argmax(y_batch) or top_3_pred[2] == np.argmax(y_batch):\n",
    "        correct = True\n",
    "    else:\n",
    "        correct = False\n",
    "    top_3_accuracy_list.append(correct)\n",
    "    batches += 1\n",
    "    if batches >= eval_generator.n/eval_generator.batch_size: #note calculate number of batches by dividing the number of images in validation set by batch_size of eval_generator\n",
    "        break\n",
    "\n",
    "image_count =EVAL_DF.shape[0]\n",
    "top_3_accuracy = sum(top_3_accuracy_list)/image_count\n",
    "print(f\"\\nTop 3 accuracy: {top_3_accuracy}\")\n",
    "\n",
    "# Parameter counts (source: https://stackoverflow.com/questions/45046525/how-can-i-get-the-number-of-trainable-parameters-of-a-model-in-keras)\n",
    "\n",
    "trainable_count = np.sum([K.count_params(w) for w in saved_model.trainable_weights])\n",
    "non_trainable_count = np.sum([K.count_params(w) for w in saved_model.non_trainable_weights])\n",
    "\n",
    "print('\\nTotal params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "print('Trainable params: {:,}'.format(trainable_count))\n",
    "print('Non-trainable params: {:,}'.format(non_trainable_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load last epoch model\n",
    "\n",
    "saved_model_2 = load_model(f\"OUTPUT_DIR/{EXPERIMENT_ID}_{ARCHIT}_{PRETRAINED_WEIGHTS}_{IMAGE_TYPE}_FINALEPOCH.h5\")\n",
    "\n",
    "# Generate dataframe of predicted labels and true labels\n",
    "\n",
    "y_pred_max_list = []\n",
    "y_true_max_list = []\n",
    "\n",
    "batches = 0\n",
    "for x_batch, y_batch in eval_generator:\n",
    "    y_pred = saved_model_2.predict(x_batch)\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "    y_pred_max_list.extend(y_pred_max)\n",
    "    y_true_max = np.argmax(y_batch, axis=1)\n",
    "    y_true_max_list.extend(y_true_max)\n",
    "    batches += 1\n",
    "    if batches >= eval_generator.n/eval_generator.batch_size: #note calculate number of batches by dividing the number of images in validation set by batch_size of eval_generator\n",
    "        break\n",
    "        \n",
    "prediction_df = pd.DataFrame({'predictions': y_pred_max_list,'labels': y_true_max_list})\n",
    "  \n",
    "# Generate confusion matrix numpy array\n",
    "confusion_mx = confusion_matrix(y_true_max_list,y_pred_max_list)\n",
    "\n",
    "# Then convert confusion matrix into pd dataframe with labels along column and along top\n",
    "class_indices = eval_generator.class_indices\n",
    "confusion_matrix_df = pd.DataFrame(confusion_mx,\n",
    "                                   index = [f\"{i}_{c}_true\" for i,c in enumerate(list(class_indices.keys()))],\n",
    "                                   columns = [f\"{i}\" for i,c in enumerate(list(class_indices.keys()))])\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix_df)\n",
    "\n",
    "# Export confusion matrix to CSV file\n",
    "confusion_matrix_df.to_csv(f\"OUTPUT_DIR/{EXPERIMENT_ID}_{ARCHIT}_{PRETRAINED_WEIGHTS}_{IMAGE_TYPE}_confusion_matrix_FINALEPOCH.csv\")\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = accuracy_score(y_true_max_list,y_pred_max_list)\n",
    "print(f\"\\nAccuracy: {overall_accuracy}\")\n",
    "\n",
    "# F1 score\n",
    "f1score = f1_score(y_true_max_list,y_pred_max_list, average='weighted')\n",
    "print(f\"\\nF1 score: {f1score}\")\n",
    "\n",
    "# Top 3 categorical accuracy\n",
    "\n",
    "top_3_accuracy_list = []\n",
    "\n",
    "batches = 0\n",
    "for x_batch, y_batch in eval_generator:\n",
    "    y_pred = saved_model_2.predict(x_batch)[0]\n",
    "    top_3_pred = np.argpartition(y_pred, -3)[-3:]\n",
    "    if top_3_pred[0] == np.argmax(y_batch) or top_3_pred[1] == np.argmax(y_batch) or top_3_pred[2] == np.argmax(y_batch):\n",
    "        correct = True\n",
    "    else:\n",
    "        correct = False\n",
    "    top_3_accuracy_list.append(correct)\n",
    "    batches += 1\n",
    "    if batches >= eval_generator.n/eval_generator.batch_size: #note calculate number of batches by dividing the number of images in validation set by batch_size of eval_generator\n",
    "        break\n",
    "\n",
    "image_count = EVAL_DF.shape[0]\n",
    "top_3_accuracy = sum(top_3_accuracy_list)/image_count\n",
    "print(f\"\\nTop 3 accuracy: {top_3_accuracy}\")\n",
    "\n",
    "# Parameter counts (source: https://stackoverflow.com/questions/45046525/how-can-i-get-the-number-of-trainable-parameters-of-a-model-in-keras)\n",
    "\n",
    "trainable_count = np.sum([K.count_params(w) for w in saved_model_2.trainable_weights])\n",
    "non_trainable_count = np.sum([K.count_params(w) for w in saved_model_2.non_trainable_weights])\n",
    "\n",
    "print('\\nTotal params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "print('Trainable params: {:,}'.format(trainable_count))\n",
    "print('Non-trainable params: {:,}'.format(non_trainable_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual evaluation:\n",
    "Plot all images in evaluation dataset set where the predicted implant model was incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For best metric epoch model\n",
    "\n",
    "plt.figure(figsize=(24, 60))  # Bigger picture\n",
    "\n",
    "subplot_idx = 0\n",
    "\n",
    "for i_batch, (x_batch, y_batch) in enumerate(eval_generator):\n",
    "    if i_batch > eval_generator.n/eval_generator.batch_size:\n",
    "        break\n",
    "    y_pred = saved_model.predict(x_batch)\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "    y_true_max = np.argmax(y_batch, axis=1)\n",
    "    pred_batch = saved_model.predict(x_batch)  # This gives us a probability for each class for all 5 samples...\n",
    "    for i_img, (x_img, y_img, pred_img) in enumerate(zip(x_batch, y_true_max, y_pred_max)):\n",
    "        if y_img != pred_img:\n",
    "            subplot_idx += 1\n",
    "            plt.subplot(12, 5, subplot_idx)  # The plot number to put the picture in\n",
    "            plt.imshow(x_img)  # Draw the picture\n",
    "            plt.title(f\"Implant: {list(eval_generator.class_indices.keys())[(y_img)]}\\nPredicted: {list(eval_generator.class_indices.keys())[pred_img]}\", color='r')\n",
    "            plt.axis('off')\n",
    "\n",
    "plt.savefig(f\"OUTPUT_DIR/{ARCHIT}_{PRETRAINED_WEIGHTS}_incorrectly_predicted_images_{EXPERIMENT_ID}_{IMAGE_TYPE}_BEST{MONITORED_METRIC}.png\", \n",
    "            dpi=500)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For final epoch model\n",
    "\n",
    "plt.figure(figsize=(24, 60))  # Bigger picture\n",
    "\n",
    "subplot_idx = 0\n",
    "\n",
    "for i_batch, (x_batch, y_batch) in enumerate(eval_generator):\n",
    "    if i_batch > eval_generator.n/eval_generator.batch_size:\n",
    "        break\n",
    "    y_pred = saved_model_2.predict(x_batch)\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "    y_true_max = np.argmax(y_batch, axis=1)\n",
    "    pred_batch = saved_model_2.predict(x_batch)  # This gives us a probability for each class for all 5 samples...\n",
    "    for i_img, (x_img, y_img, pred_img) in enumerate(zip(x_batch, y_true_max, y_pred_max)):\n",
    "        if y_img != pred_img:\n",
    "            subplot_idx += 1\n",
    "            plt.subplot(12, 5, subplot_idx)  # The plot number to put the picture in\n",
    "            plt.imshow(x_img)  # Draw the picture\n",
    "            plt.title(f\"Implant: {list(eval_generator.class_indices.keys())[(y_img)]}\\nPredicted: {list(eval_generator.class_indices.keys())[pred_img]}\", color='r')\n",
    "            plt.axis('off')\n",
    "\n",
    "plt.savefig(f\"OUTPUT_DIR/{ARCHIT}_{PRETRAINED_WEIGHTS}_incorrectly_predicted_images_{EXPERIMENT_ID}_{IMAGE_TYPE}_FINALEPOCH.png\", \n",
    "            dpi=500)  \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
